# -*- coding: utf-8 -*-
"""Capstone - Telecommunication Customer Churn - With Comments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t0uyzDSWdWD6JFdpu33Kpbxco6-Ads--
"""

#Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Import Libraries for Machine Learning Model

from sklearn import metrics

from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score, confusion_matrix, classification_report, accuracy_score, confusion_matrix
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from imblearn.combine import SMOTEENN
from sklearn.ensemble import RandomForestClassifier

#Load the dataset and print the head
df = pd.read_csv('TelcoCustomerChurn.csv')
df.head()

#Find out df size

print("Size - Rows and Columns")
df.shape

#Find the column names
print("\nColumn Names")
df.columns

#Describe Database
df.describe()

"""```
From describing the data, we can see that:
The average tenure of customers is 32 months.

The average monthly charges are just under $65
and the highest 25% bay over $89 a month.

The minimum payment was just over $18, and the max
was over $118, so there's a wide range in monthly payment ammounts.
```


"""

#Find the number of customers, and how many have churned.
sns.countplot(x='Churn', data=df, palette='husl')
plt.title('Number of Churned Customers', fontsize = 12)
plt.show()

#Check what data type each column is
print("\nData Types")
df.dtypes

"""

```
Here we can notice that 'TotalCharges' is an object type, this doesn't make sense
as it represents the total charges in US currency.
```

"""

#Check for missing values
df.isnull().sum()

"""

```
At first glance, it doesn't look like there are any missing values.
But remember the data types, and how 'TotalCharges' was an Object type, not a float.

We need to convert it to a number type to actually catch any null values.
```

"""

#df_numeric_values = df.copy()
#df_numeric_values['TotalCharges'] = pd.to_numeric(df_numeric_values['TotalCharges'], errors='coerce')

#df_numeric_values.isnull().sum()

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

df.isnull().sum()

#Look at the 11 missing charges in 'Total Charges'
#df_numeric_values[df_numeric_values['TotalCharges'].isnull()]

df[df['TotalCharges'].isnull()]

"""

```
Notice that for everyone who's 'Total Charges' is null, they also have a tenure of 0.
This could be because they just started and they haven't paid yet.

These records won't tell us very much about customer patterns, and as there's only 11
records out of over 7,000, we'll be able to drop them without much impact.

While we're at it, we also don't need 'customerID' as it's not relevant to training the
model and is potentially personal identification.
```

"""

#Drop the null values, and the customerID column
#df_numeric_values.dropna(inplace=True)

#df = df_numeric_values.drop('customerID', axis=1, inplace=True)

df.dropna(inplace=True)

df = df.drop('customerID', axis=1)

#Check each column's null counts
df.isna().sum()

df.shape

#Divide the tenure into bins and assign them to groups
#Used ChatGPT here to help debug, and figure out edge cases

labels = ["{0} - {1}".format(i,i+5) for i in range (1,72,6)]

df['TenureGroup'] = pd.cut(df['tenure'], range(1,74,6), right=False, labels=labels)

df['TenureGroup'].value_counts()

#Plot out the number of customers and whether they churned in bins of 6 months
plt.figure(figsize=(15, 8))
sns.countplot(x=df['TenureGroup'],hue='Churn',data=df, palette='rocket_r')
plt.title('Number of Churned Customers by Tenure (6 month Intervals)', fontsize = 12)
plt.show()

"""

```
Now that we have transformed Tenure into Tenure Group,
```

"""

#Drop Tenure and just keep TenureGroup

"""### **EXPLORATORY DATA ANALYSIS**"""

#Show count of each category for churn
df['Churn'].value_counts()

#Find the number of customers, and how many have churned.
sns.countplot(x='Churn', data=df, hue= 'Churn', palette='rocket_r')
plt.title('Number of Churned Customers', fontsize = 12)
plt.show()

#Change Churn variable into a binary value
df['Churn'].replace(["Yes","No"], [1,0],inplace=True)
100*df[df['Churn']==1].shape[0]/df.shape[0]

"""


```
With a quick calculation, and from what we can see in the chart that around 26.6%
of the dataset is churn data, and not 50/50. This means that our data is imbalanced.
```


"""

#Find what percent of the dataset is Senior Citizens

100*df[df['SeniorCitizen']==1].shape[0]/df.shape[0]

"""

```
Only around 16% of the dataset are senior citizens, a majority
of our data comes from younger people.
```

"""

#Plot all the important countplots, excluding numerical columns for charges

for i, column in enumerate(df.drop(columns=['Churn','TotalCharges','MonthlyCharges','tenure'])): #Excludes TotalCharges and MontlyCharges
    plt.figure(i,figsize=(15,6))
    sns.countplot(data=df,x=column,hue='Churn',palette='rocket_r')
    plt.show()

#One hot encoding
df = pd.get_dummies(df)

df.head(2)

#Create a correlation matrix of the dataset

plt.figure(figsize=(12,12))
sns.heatmap(df.corr(),cmap='coolwarm')

#Sort the most positively to negatively correlated variables with 'churn'
df.corr()['Churn'].sort_values(ascending = False)

#Plot a bar chart of all the variable sorted from most correlated to least.
plt.figure(figsize=(10,8))
df.corr()['Churn'].sort_values(ascending = False).drop(['Churn']).plot(kind='bar')
plt.show()

"""### CREATE MACHINE LEARNING MODELS
Prepare, Train-Test Split, Balance Data, Train the Model, and Evaluate.

# Logistic Regression
"""

# Split features and target
X = df.drop('Churn', axis=1)
y = df['Churn']

#Train-test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""

```
I originally didn't balance the dataset, and as a result my model, understandably
had issues. In my research I learned about SMOTEENN, which allows us to easily balance the dataset.
```

"""

#Use smoteenn to balance the data
smote_enn = SMOTEENN(random_state=42)
X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)

#Train the Logistic Regression Model
log_model = LogisticRegression(max_iter=1000, solver='liblinear')
log_model.fit(X_resampled, y_resampled)

#Evaluate the Model
y_pred = log_model.predict(X_test)

# Metrics
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

#Plot Confusion Matrix
y_pred = log_model.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm', xticklabels=["No Churn", "Churn"], yticklabels=["No Churn", "Churn"])

plt.title('Confusion Matrix - Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()